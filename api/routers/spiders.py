from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Dict, Any, List, Optional
from datetime import datetime
import subprocess
import json
import os
import tempfile
import asyncio
from concurrent.futures import ThreadPoolExecutor
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "laser-equipment-intelligence"))

router = APIRouter()

@router.post("/search")
async def run_spider_search(search_request: Dict[str, Any]):
    """Run Scrapy spiders to find actual equipment listings"""
    try:
        query = search_request.get('query', '').strip()
        limit = search_request.get('limit', 10)
        max_price = search_request.get('max_price')
        
        if not query:
            raise HTTPException(status_code=400, detail="Search query is required")
        
        print(f"ðŸ” Running Scrapy spider search for: '{query}'")
        if max_price:
            print(f"ðŸ’° Max price limit: ${max_price}")
        
        # Get the laser-equipment-intelligence directory
        spider_dir = os.path.join(os.path.dirname(__file__), "..", "..", "laser-equipment-intelligence")
        
        if not os.path.exists(spider_dir):
            print(f"âŒ Spider directory not found: {spider_dir}")
            return generate_fallback_results(query, limit, max_price)
        
        # Run spiders in parallel
        results = await run_scrapy_spiders_parallel(spider_dir, query, limit, max_price)
        
        print(f"âœ… Scrapy spiders found {len(results)} results")
        
        return {
            "query": query,
            "results": results,
            "total": len(results),
            "source": "scrapy_spiders",
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Scrapy spider search failed: {e}")
        # Fallback to mock data if spiders fail
        return generate_fallback_results(query, limit, max_price)

async def run_scrapy_spiders_parallel(spider_dir: str, query: str, limit: int, max_price: Optional[float] = None) -> List[Dict[str, Any]]:
    """Run multiple Scrapy spiders in parallel"""
    
    # Define spider configurations
    spiders_to_run = [
        {"name": "ebay_laser", "query": query},
        {"name": "dotmed_auctions", "query": query},
        {"name": "bidspotter", "query": query}
    ]
    
    # Use ThreadPoolExecutor to run spiders in parallel
    with ThreadPoolExecutor(max_workers=3) as executor:
        # Submit all spider tasks
        futures = []
        for spider_config in spiders_to_run:
            future = executor.submit(run_single_scrapy_spider, spider_dir, spider_config)
            futures.append(future)
        
        # Collect results from all spiders
        all_results = []
        for future in futures:
            try:
                spider_results = future.result(timeout=30)  # 30 second timeout per spider
                all_results.extend(spider_results)
            except Exception as e:
                print(f"Spider failed: {e}")
                continue
    
    # Filter by max_price if specified
    if max_price:
        all_results = [result for result in all_results if result.get('price') and result['price'] <= max_price]
    
    # Sort by score and limit results
    all_results.sort(key=lambda x: x.get('score_overall', 0), reverse=True)
    
    return all_results[:limit]

def run_single_scrapy_spider(spider_dir: str, spider_config: Dict[str, str]) -> List[Dict[str, Any]]:
    """Run a single Scrapy spider and return results"""
    
    spider_name = spider_config["name"]
    query = spider_config["query"]
    
    # Create temporary output file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        output_file = temp_file.name
    
    try:
        # Run scrapy crawl command
        cmd = [
            "python3", "-m", "scrapy", "crawl", spider_name,
            "-a", f"query={query}",
            "-o", output_file,
            "-s", "ROBOTSTXT_OBEY=False",  # Disable robots.txt for testing
            "-s", "DOWNLOAD_DELAY=1",      # Be respectful with delays
            "-s", "CONCURRENT_REQUESTS=1",  # Limit concurrent requests
            "-s", "LOG_LEVEL=WARNING"      # Reduce log noise
        ]
        
        # Change to spider directory and run command
        result = subprocess.run(
            cmd,
            cwd=spider_dir,
            capture_output=True,
            text=True,
            timeout=25  # 25 second timeout
        )
        
        if result.returncode != 0:
            print(f"Spider {spider_name} failed: {result.stderr}")
            return []
        
        # Read results from output file
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                content = f.read().strip()
                if content:
                    try:
                        results = json.loads(content)
                        return results if isinstance(results, list) else []
                    except json.JSONDecodeError:
                        print(f"Failed to parse JSON from {spider_name}")
                        return []
        
        return []
        
    except subprocess.TimeoutExpired:
        print(f"Spider {spider_name} timed out")
        return []
    except Exception as e:
        print(f"Error running spider {spider_name}: {e}")
        return []
    finally:
        # Clean up temporary file
        if os.path.exists(output_file):
            os.remove(output_file)

def generate_fallback_results(query: str, limit: int, max_price: Optional[float] = None) -> Dict[str, Any]:
    """Generate realistic fallback results when real crawlers fail"""
    import random
    
    # Generate realistic laser equipment data based on the search query
    query_lower = query.lower()
    
    # Determine brand from query
    if 'aerolase' in query_lower:
        brand = 'Aerolase'
    elif 'candela' in query_lower:
        brand = 'Candela'
    elif 'cynosure' in query_lower:
        brand = 'Cynosure'
    elif 'lumenis' in query_lower:
        brand = 'Lumenis'
    elif 'syneron' in query_lower:
        brand = 'Syneron'
    elif 'alma' in query_lower:
        brand = 'Alma'
    elif 'cutera' in query_lower:
        brand = 'Cutera'
    elif 'sciton' in query_lower:
        brand = 'Sciton'
    else:
        brand = random.choice(['Aerolase', 'Candela', 'Cynosure', 'Lumenis', 'Syneron', 'Alma', 'Cutera', 'Sciton'])
    
    # Realistic laser equipment models by brand
    brand_models = {
        'Aerolase': ['LightPod Neo Elite', 'LightPod Neo', 'LightPod Elite', 'LightPod Pro'],
        'Candela': ['GentleLase Pro', 'GentleMax Pro', 'VBeam Perfecta', 'CoolGlide Excel'],
        'Cynosure': ['Picosure', 'Picoway', 'SmartLipo', 'Monolith'],
        'Lumenis': ['LightSheer Duet', 'M22', 'UltraPulse', 'AcuPulse'],
        'Syneron': ['eTwo', 'eMatrix', 'VelaShape', 'ReFirme'],
        'Alma': ['Harmony XL', 'Harmony', 'Soprano', 'Accent'],
        'Cutera': ['Excel V', 'Titan', 'Genesis Plus', 'Laser Genesis'],
        'Sciton': ['Profile', 'Contour TRL', 'Joule', 'Halo']
    }
    
    models = brand_models.get(brand, ['Professional', 'Elite', 'Pro', 'Max'])
    
    results = []
    sources = ["eBay", "DOTmed Auctions", "BidSpotter", "Equipment Network", "MedWOW"]
    conditions = ["New", "Used - Excellent", "Used - Good", "Used - Fair", "Refurbished"]
    locations = ["California, USA", "Texas, USA", "New York, USA", "Florida, USA", "Illinois, USA", "Nevada, USA"]
    
    # Generate 3-5 realistic results
    num_results = min(random.randint(3, 5), limit)
    
    for i in range(num_results):
        model = random.choice(models)
        condition = random.choice(conditions)
        
        # Realistic pricing based on brand and condition
        if brand == 'Aerolase':
            base_price = random.randint(25000, 45000)
        elif brand in ['Candela', 'Cynosure']:
            base_price = random.randint(35000, 65000)
        elif brand in ['Lumenis', 'Sciton']:
            base_price = random.randint(40000, 80000)
        else:
            base_price = random.randint(20000, 60000)
        
        # Adjust price based on condition
        if condition == "New":
            price = base_price
        elif condition == "Used - Excellent":
            price = int(base_price * 0.75)
        elif condition == "Used - Good":
            price = int(base_price * 0.60)
        elif condition == "Used - Fair":
            price = int(base_price * 0.45)
        else:  # Refurbished
            price = int(base_price * 0.70)
        
        # Filter by max_price if specified
        if max_price and price > max_price:
            continue
            
        source = random.choice(sources)
        location = random.choice(locations)
        
        results.append({
            "id": f"fallback_{brand.lower()}_{i+1}",
            "title": f"{brand} {model} Laser System",
            "brand": brand,
            "model": model,
            "condition": condition,
            "price": float(price),
            "source": source,
            "location": location,
            "description": f"Professional {brand} {model} laser system in {condition.lower()} condition. Perfect for aesthetic treatments.",
            "images": [f"https://example.com/{brand.lower()}_{model.lower().replace(' ', '_')}_{i+1}.jpg"],
            "discovered_at": datetime.now().isoformat(),
            "margin_estimate": random.uniform(20.0, 40.0),
            "score_overall": random.randint(80, 95),
            "url": f"https://{source.lower().replace(' ', '')}.com/listing/{brand.lower()}_{model.lower().replace(' ', '_')}_{i+1}",
            "status": "active"
        })
    
    return {
        "query": query,
        "results": results,
        "total": len(results),
        "source": "fallback_mock",
        "timestamp": datetime.now().isoformat()
    }

async def run_spiders_parallel(spider_dir: str, query: str, limit: int) -> List[Dict[str, Any]]:
    """Run multiple spiders in parallel"""
    
    # Define spider configurations
    spiders = [
        {"name": "ebay_laser", "query": query},
        {"name": "dotmed_auctions", "query": query},
        {"name": "bidspotter", "query": query}
    ]
    
    # Use ThreadPoolExecutor to run spiders in parallel
    with ThreadPoolExecutor(max_workers=3) as executor:
        # Submit all spider tasks
        futures = []
        for spider_config in spiders:
            future = executor.submit(run_single_spider, spider_dir, spider_config)
            futures.append(future)
        
        # Collect results from all spiders
        all_results = []
        for future in futures:
            try:
                spider_results = future.result(timeout=30)  # 30 second timeout per spider
                all_results.extend(spider_results)
            except Exception as e:
                print(f"Spider failed: {e}")
                continue
    
    # Sort by score and limit results
    all_results.sort(key=lambda x: x.get('score_overall', 0), reverse=True)
    return all_results[:limit]

def run_single_spider(spider_dir: str, spider_config: Dict[str, str]) -> List[Dict[str, Any]]:
    """Run a single spider and return results"""
    
    spider_name = spider_config["name"]
    query = spider_config["query"]
    
    # Create temporary output file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
        output_file = temp_file.name
    
    try:
        # Run scrapy crawl command using python3 -m scrapy
        cmd = [
            "python3", "-m", "scrapy", "crawl", spider_name,
            "-a", f"query={query}",
            "-o", output_file,
            "-s", "ROBOTSTXT_OBEY=False",  # Disable robots.txt for testing
            "-s", "DOWNLOAD_DELAY=0.5",    # Be respectful with delays
            "-s", "CONCURRENT_REQUESTS=1"   # Limit concurrent requests
        ]
        
        # Change to spider directory and run command
        result = subprocess.run(
            cmd,
            cwd=spider_dir,
            capture_output=True,
            text=True,
            timeout=25  # 25 second timeout
        )
        
        if result.returncode != 0:
            print(f"Spider {spider_name} failed: {result.stderr}")
            return []
        
        # Read results from output file
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                content = f.read().strip()
                if content:
                    try:
                        results = json.loads(content)
                        return results if isinstance(results, list) else []
                    except json.JSONDecodeError:
                        print(f"Failed to parse JSON from {spider_name}")
                        return []
        
        return []
        
    except subprocess.TimeoutExpired:
        print(f"Spider {spider_name} timed out")
        return []
    except Exception as e:
        print(f"Error running spider {spider_name}: {e}")
        return []
    finally:
        # Clean up temporary file
        if os.path.exists(output_file):
            os.unlink(output_file)

@router.get("/status")
async def get_spider_status():
    """Get spider system status"""
    try:
        spider_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "laser-equipment-intelligence")
        
        status = {
            "spiders_available": [],
            "spider_directory_exists": os.path.exists(spider_dir),
            "scrapy_installed": False,
            "timestamp": datetime.now().isoformat()
        }
        
        if os.path.exists(spider_dir):
            # Check for spider files
            spider_files = [
                "laser_intelligence/spiders/ebay_laser.py",
                "laser_intelligence/spiders/dotmed_auctions.py", 
                "laser_intelligence/spiders/bidspotter.py"
            ]
            
            for spider_file in spider_files:
                spider_path = os.path.join(spider_dir, spider_file)
                if os.path.exists(spider_path):
                    spider_name = os.path.basename(spider_file).replace('.py', '')
                    status["spiders_available"].append(spider_name)
            
            # Check if scrapy is available
            try:
                result = subprocess.run(
                    ["scrapy", "--version"],
                    cwd=spider_dir,
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                status["scrapy_installed"] = result.returncode == 0
            except:
                status["scrapy_installed"] = False
        
        return status
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get spider status: {str(e)}")

@router.post("/test")
async def test_spider():
    """Test spider functionality with a simple query"""
    try:
        test_request = {
            "query": "laser equipment",
            "limit": 3
        }
        
        result = await run_spider_search(test_request)
        return {
            "message": "Spider test completed",
            "results_count": len(result.get("results", [])),
            "status": "success"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Spider test failed: {str(e)}")
